 After looking at the basic algorithms and techniques like the greedy techniques, today we come to an advanced technique called dynamic programming. Dynamic programming is an algorithm design method. It is used when the solution to a problem can be obtained by selecting a solution to some sub problems. This is a very loose way of putting it, but we will set up all the formal details in the lecture today. Most importantly the role of this slide is to tell you that dynamic programming techniques are very fundamentally different from greedy algorithmic techniques. Often in the greedy algorithms there is this power of making a greedy choice which we have seen in earlier lectures. And the greedy choice identifies a single recursive sub problem to solve. Let us look at the shortest path problem as an example. What you see is a layered graph. It has four vertices, S, A, B, and T and you also see the edge cause. And if one uses the greedy technique to find the length of the shortest path from S to T by being greedy. In the sense that from S we choose the edge of least weight. Then from A we choose an edge of the least weight and from B we choose an edge of the least weight. Then we can conclude that the greedy algorithm will output a path of length 8. However it is quite clear that the length of the shortest path is indeed 9. In this example the, in this example the algorithm does output the shortest path. But let us look at the shortest path in a slightly more complicated looking graph. If we are greedy then the greedy algorithm will choose the path which goes from S to the vertex A because the edge weight 1 is the shortest among all the edges that go out of S. And from A it chooses the cheapest edge weight which is 4 and takes it to the vertex D and D it chooses the shortest edge whose cost is 18. And we conclude that the greedy algorithm outputs a path whose length is of 23 units as opposed to the shortest path which is from S to C, C to F and F to T and the cost of this path is indeed 9. Therefore it is clear that to solve the shortest path problem a greedy strategy does not work. Let us just look at the dynamic programming approach and let us also make some observations about the shortest path. The length of the shortest path from S to T is obtained by calculating the shortest paths via the vertices A, B and C respectively and then picking the shortest path among the three in this example. It is quite clear S and S has three neighbors only which are called A, B and C respectively. Now if we know the shortest path from A to T and the shortest path from B to T and the shortest path from C to T respectively then adding up the appropriate edge cost from S and then choosing the minimum will very clearly give the shortest path from S to T. Therefore what we have now observed is that to solve the shortest path problem we need to solve recursive versions of the shortest path problem from neighboring vertices. Let us just run the example on let us just look at the formulation of the distance from S to T or in other words the shortest path from S to T. Clearly as we just discussed distance from S to T is minimum over 1 plus the distance from A to T the 1 comes because of the edge rate from S to A is 1, 2 plus distance from B to T because edge rate from S to B is of cost 2 and 5 plus the distance from C to T the edge rate from S to C being 5. Indeed if we know the shortest paths from A to T, B to T and C to T respectively this minimum gives us the shortest path from S to T. Now the distance from A to T the part of the relevant part of the graph is shown here the distance from A to T as you can see is the minimum of 4 times the distance 4 plus the distance from D to T plus the distance from A to T is given by the minimum of 4 plus the distance from D to T and 11 plus the distance from E to T where D and E are the only neighbors of A. We can similarly now formulate the distance from B to T the distance from C to T and distance from S to T can then be used to calculate. So the distance from B to T is again 9 plus the distance from D to T the distance from B to T is given by the minimum of the 3 distances via D via E and via F respectively. Similarly the distance from C to T there is just a single path that goes through F and this is given as 2 plus the distance from F to T. One can now observe that the distance from S to T will indeed choose the path that goes via C which is of cost 9 because that is where the minimum is attained. What we have done in the slide so far is to observe that a recursive formulation of the distance function in terms of distances from other vertices to the destination is helpful in computing the shortest path. It is also possible to observe that one can calculate the distances from the recursive formulations which we have just observed. One can observe the distance from S to D an intermediate vertex not necessarily T is the minimum of the distance from S to A plus the distance from A to T and similarly the distance from S to B plus the distance from B to T. In other words we are taking the minimum of the distances via intermediate points A and B respectively. Similarly the distance from S to E and distance from S to F are also presented. Now that we have seen this example let us just formally state dynamic programming and its purposes. It is used for solving optimization problems which are minimization problems or maximization problems and in all these problems a set of choices must be made to get an optimum solution. In the examples that we looked at the appropriate edges must be chosen to find the length of the shortest path from S to T. The choices are the edges that must be taken by a shortest path from S to T. There may be many such paths for example in an undirected graph you every path with the least number of edges from S to T is indeed a shortest path and our goal is to find an optimal solution. In other words to find the optimal set of choices. When we design a dynamic programming algorithm the approach that we take is to first understand the structure of the optimal solution and characterize this optimal solution. By characterisation we mean that we identify certain very important properties of solutions which necessitate them to be optimal solutions. That is what we mean by a characterisation. After characterising the structure of an optimal solution we then recursively go ahead and write a recursive expression for the optimal solution and then compute the value of the optimum solution in a bottom of fashion. The last computation is indeed done via a program but the first two are analytical exercises and the computation indeed implements the recursion. As an example we study the assembly line scheduling exercise which is an introductory exercise and dynamic programming in the standard textbook where carbon, lysos and and rivus and the pictures that you see are taken from the textbook. Assembly line scheduling can be very intuitively visualised. It is a very important problem in the area of manufacturing. The framework is of an automobile factory. The framework is of an automobile factory which has two assembly lines. Each line has end stations and processing stations. On the first line the processing stations are called S1 1 to S1N and in the second line the processing stations are called S2 1 to S2N respectively. There are end stations. Now corresponding to the stations each of the stations have a certain processing time. In particular for the station S1J that is a J station on the line 1 the processing time is A1J units of time and similarly at the station S2J the processing time is A2J units of time. The entry times into the station are E1 and E2 respectively. The entry times into the lines that is the entry into the first station S1 1 and the entry into the first station S2 1 are E1 and E2 respectively and the exit times from the stations are X1 and X2 respectively. One can assume that in this automobile factory a car chassis enters one of these two assembly lines and then goes through from one station to another and exits with a completed vehicle. Along the way the car station the car along the way a car can stay on the same line by going to the next station in the line and it does not pay any cost in terms of time duration or it could transfer to the other line with a cost which is Tij units of time. In other words if a car gets processed at station S1 1 if a car gets processed at station S1 1 then it can either go instantaneously to station S1 2 or in T11 units of time it can go to the station A22. So, what is the problem in this whole exercise? The problem in this whole exercise is that a chassis must be rooted through this network. It must visit every station it does not matter in which line it gets processed but it must get processed at every station and exit in a minimum amount of time. This is for a single chassis. Now one of the ways of solving the problem is to actually try all the possible ways by which a chassis could go through the assembly lines and it is very clear that there are two power n of them and computationally spending two power n units of time to actually find out which is the most optimal sequence is not an efficient approach. So, the implementation of this idea is essentially to have a bit vector a binary vector which essentially says if a certain bit for example in this case at step 3 if the bit is set to 0 then the vehicle goes through station 4 the vehicle goes through station 2 in step 3 and if it is one the vehicle goes through station 1 in step 3. The implementation of this brute force algorithm is to keep a bit vector where 0 indicates that the vehicle 0 indicates that the chassis will go through line 2 and 1 indicates that the chassis will go through line 1 and once these are fixed the time that the chassis spends in the factory can be easily calculated. It is just the length of the path that goes through these stations the sum of the time durations on the edges will tell us the total time that the vehicle will tell us the total time the chassis spends in the factory before exiting and then we can choose the bit vector which gives the least solution and indeed we would have solved the desired problem. However, we would have spent two power n units of time to avoid this inefficiency in terms of the amount of time that has been spent by the algorithm in finding the optimal path we try and understand the structure of an optimal solution which is the first step in the design of a dynamic programming algorithm. In this graphic we indeed have highlighted the optimum path in dark color we will see how the algorithm will indeed find this particular path. To understand the optimal solution we invent a parameter of interest. Let us look at the station J on the first line and let us ask if a shortest path goes through S1J then do we have some information as through shortest paths to other stations that preceded it that are computed in that shortest path. To understand the structure of the optimal solution we ask the following question if there are two if we look at the set of all possible ways from the start to exit via S1J there are two possibilities for a path that goes via S1J the previous vertex before the station S1J could have been the station S1J minus 1 and a direct transfer to S1J or the previous station could have been S2J minus 1 that is the station on the second line and the transfer over to S1J. We now make the observation that the fastest way through S1J is through S1J minus 1 in other words if we consider all the paths go from the start to exit which go through S1J and among these if we consider the path which is the fastest and let us say that such a shortest path or such a fastest path indeed goes through S1J minus 1. Then it is also clear that such a path must have taken the fastest way from start through S1J minus 1. The reason is very straightforward, indeed if there is a faster way through S1J minus 1 then we could have used it and found a faster path through S1J. This is really the optimal structure or it is a structure of an optimal solution. The structure of an optimal solution is the fastest way through S1J contains with it an optimal solution to the fastest way through S1J minus 1 or S2J minus 1 whichever is there in the fastest way through S1J. This is referred to as the optimal substructure property. It is this property that is used to write down the recurrence for the length of the shortest path from S to T or the fastest path from S to T. So what we are going to do now which is a second step in the process of dynamic programming is to write down the optimal solution in terms of the optimal solutions to subproblems. Let us introduce some necessary notation. Let us assume that F star is the fastest time to go through the whole factory and let Fij denote the fastest time to go from the starting point through the station SiJ. Therefore F star is very clear F star is a minimum of the two terms which is F1 of N plus X1 that is the time taken to exit from line 1, F2 of N plus X2 which is the time taken to exit from line 2. Therefore this expression tells us that we need to compute F1 of N and F2 recursively. Let us start off with the base case. Let us consider F1 of N. Let us consider F1 of 1 that is through station 1. What is the fastest path that goes through station 1 on line 1? It is a time taken to enter line 1 and get processed at the first station. In this case as you can see it is 9 units of time but the generic formula that we can write down is that F1 of N is E1 plus A11. This is the time taken to enter into line 1 and the time taken to be processed at the first station on line 1. Similarly F2 of 1 is a time taken to enter into line 2 plus the time taken to be processed at station 1 on line 2. The formula I have written there F1 of 1 is E1 plus A11 and F2 of 1 is E2 plus A21. In general we can write down a recursive specification of F1J or F2J. They are symmetric. Let us just look at the case for F1J. So the fastest way through Sij has only two possibilities. The two possibilities are use the fastest way to come to station J-1 on line 1. Use the fastest way to cross station J-1 on line 1 and then be processed at station J on line 1 or use the fastest way to arrive at station J-1 on line 2 then transfer to line 1 and then get processed at station J on line 1. Therefore F1 of J is a minimum of F1 of J-1 plus A1 of J comma F2 of J-1 plus the time to transfer out of the J-1 station on line 2 to the J-1 line 1 that is T2J-1 plus A1J. Observe that this is a recursive specification of F1 of J. The recursive specification of F2 of J is also similar and this basically tells us the recursive solution of interest for every station on each of the two lines. Having formulated this recursive specification of the time taken to cross station J on line 1 and the time taken to cross station J on line 2 and this we have done for every J between 1 and N. We now wonder how what it is to calculate these values from these formulae and this is the exercise of computing the optimum solution and one of the ways of solving the optimum solution is to solve this in a top-down fashion and clearly if one tries to compute F1 of N and F2 of N in a top-down fashion at every step we are taking the minimum of two quantities and it is clear that the running time of this algorithm which is to evaluate this recurrence in the top-down fashion will take an exponential time. In other words it will take two power N units of time which does not seem to be a significant improvement which is not an improvement at all over the simple algorithm that we started out with the brute force algorithm. On the other hand if one uses the bottom-up approach one can indeed observe that the values of interest F1 of N and F2 of N can be very easily calculated. The main observation that we make is that for J greater than or equal to 2 the value of FI of J depends only on F1 of J minus 1 and F2 of J minus 1. In other words the fastest way of crossing station J on either line depends only on the fastest way of crossing station J minus 1 1 line 1 and the fastest way of crossing station J minus 1 on line 2 and we compute the values of Fij as described here in increasing order of J. Let us look at the time taken to cross station 1 which we know. In this example we can see that the fastest way of crossing station 1 on line 1 is 9 units of time. On station 2 it is 12 units of on line 2 the fastest way of crossing station 1 takes 12 units of time. After this let us look at station 2 the fastest way of crossing station 2 on line 1 is 18 units of time and the number in parenthesis says which station was the on which line was the previous station. In this case the fastest way of crossing station 2 on line 1 is via station 1 on line 1 itself which is an instantaneous transfer to station 2 and then 9 units of processing time. Also the fastest way of crossing station 2 on line 2 is to actually go through station 1 in line 1 and then transfer to station 2 on line 2 and be processed at station 2 on line 2. Observe that this is 16 units of time as opposed to 12 units of time plus 2 units of time to transfer as opposed to 12 units of time at station 1 on line 2 and then instantaneously be transferred to station 2 on line 2 and then use 5 units of time which makes it 23 units of time. Similarly, at station 3 on line 1 observe that the fastest way of crossing station 3 on line 1 goes through station 2 on line 2 and uses 20 units of time as opposed to crossing station 3 on line 2 which takes 22 units of time. Similarly crossing station 4 on line 4 takes 24 units of time via station 3 on line 1 itself and on line 2 it takes 25 units of time via station 3 on line 1 and crossing station 5 takes 32 units of time via station 4 on line 1 and 30 units of time via station 4 on line 2 and then the exit takes 3 and 5 units of time respectively. Therefore, the quickest way of getting from entry to exit is 35 units of time via station 5 on line 1. Having gone through this whole exercise of recursive of identifying the optimal substructure and recursively specifying the distance function and observing that it can be computed in the bottom of fashion. Computing the optimal route that is the sequence of stations through which the shortest path passes through is left as an exercise. The next example of dynamic programming which we will study is the problem of optimal matrix strain multiplication which will be done when we meet next. Thank you.