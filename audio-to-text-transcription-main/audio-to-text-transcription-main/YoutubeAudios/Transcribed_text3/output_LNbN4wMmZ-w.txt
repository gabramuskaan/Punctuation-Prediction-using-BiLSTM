 So, today's lecture is about greedy algorithms and specifically we will be studying one algorithm for a problem called job scheduling. The initial part of the presentation will present you the motivation for greedy algorithms by a very simple algorithm, very simple example. The example is the change making example. So, let us consider the following problem where for an example let us consider we have 832 rupees and using as few nodes and coins as possible we should keep 832 rupees let us say in our purse. In general we know that there are 9 denominations of rupees in the Indian currency. So, let us call these denominations d1 to d9 indeed they take the values from 1500, 150, 20, 10, 5, 2 and 1. Naturally, we wish to carry as few nodes and coins as possible and the generic question is given M rupees identify 9 variables these are integer valued variables positive integer valued variables such that the M rupees can be written as a sum of x1 times the first denomination d1 x2 times the second denomination d2. So, 1 up to x9 times the 9th denomination d9. For example 832 can be written as 1500 rupees plus 300 rupees 120 rupees 1 10 rupee and 1 2 rupee coin. In general do we have a natural approach here to solve this problem. The natural approach seems to be that iteratively we select the largest denomination which is available which is smaller than the current amount. Let us observe this by the example itself 832 is the current amount and the largest denomination which is available to us is a 500 rupee note. So, we pick up 500 rupees and the balance that is to be expressed is 332. Now, 332 the smallest the largest denomination smaller than 332 is a 100 rupee note. Naturally we pick 3 times 100. We pick 100 once then the balance is 232 the smallest denomination large the largest denomination smaller than this amount is another 100 and so on. So, we pick 3 times 100 rupees then we pick a 20 rupee note because the balance that is to be expressed is 32 rupees and so on. So, the generic idea seems to be that we select the largest denomination that is available such that it is smaller than the current amount that we need to express. Does this always work independent of the denominations? This is an interesting question. Observe that we did our exercise with the Indian currency with a fixed set of denominations. Let us imagine whether this approach will work for any arbitrary set of denominations in some currency. Let us assume that there is a country with currency right. Whose denominations are 1, 3 and 4 respectively and there is no other denomination. Would our natural generic approach yield the optimum solution? Let us see. Let us consider the simplest case of 6. So, if we apply our generic approach we will pick the largest denomination smaller than the amount which is 6 therefore, we will pick 4. Another which we are left with 2 which can be expressed as 1 plus 1. So, our solution would be 4 plus 1 plus 1. On the other hand it is clear in this very simple example that the best denomination possible with the best representation of 6 possible in this currency is 3 plus 3. Therefore, it is clear that this generic algorithm which seems to be a very natural approach does not work for all currencies. It depends only seems to depend on the denominations. A definite exercise which we will not address in this class is does this work for the Indian currency and if so why does it? We will not address this but this is left as an exercise for the interested student. This takes us to this whole idea of greedy algorithms. So, observe that our algorithmic approach is a greedy approach. So, we choose the locally best possible choice. So, given a certain amount we choose the largest possible denomination without exceeding the current amount. So, this is in some sense a greedy choice hoping that this will eventually lead to a optimal solution as we have seen it does not work for all currencies. So, therefore, greedy algorithms should have an optimal substructure. What is an optimal substructure? We should be able to guarantee the existence of an optimal solution to the problem such that this optimal solution also contains optimal solutions to the sub problems. This is a very important concept. You have a problem and you also have sub problems of the problem. This is best illustrated by the shortest path problem in any network. Let us assume that the network is undirected and let us think of the shortest path problem between a source and destination. Observe that if you pick any shortest path between the source and destination and if you pick any pair of vertices on the shortest path not necessarily source and destination, it is clear that the shortest path contains the shortest paths between the intermediate points. So, very natural property of shortest paths. So, this is the optimal substructure. Observe that no algorithmic choice is here it is just a property of an optimum solution. The greedy choice property is an algorithmic choice. It says that you can obtain a globally optimum solution by making locally optimal choices. We have seen this example of the change making problem and we wonder whether it has the greedy choice property. We have already observed that if the denomination is denominations are 1, 3 and 4 respectively. If you take an optimum solution, let us take the optimum solution for 6 in this example, which is 3 plus 3. Note that every subset of notes is the optimum change for its sum. So, in this case every subset is just the node 3 itself which is the optimum change for itself. So, this is easy. The greedy choice on the other hand is not satisfied as we have already seen. We will select 4, then we selected 1 followed by a 1 and this turned out to be suboptimal. So, we repeat this exercise. What the greedy choice work for the Indian currency? Indeed in this slide we have observed that the optimal substructure seems to work for any currency. However, like I said earlier we will not address it in this presentation because the problem is not as easy as it seems. It is very easy to state it, but it is very closely related to problems which after many years of research have not yielded to efficient algorithms. For example, there is a problem called the NAPSAC problem. It does not have efficient algorithms to date, it is believed not to have efficient algorithms. There is also the optimum denomination problem which is actually a problem faced by currency designers. Where the question is what is the best denomination to ensure that for every number you get the smallest possible change by our approach. So, therefore, let us ask a slightly different question are there problems for which greedy algorithms result in optimum solutions? This is the focus. The change making example was a natural example to visualize the greedy algorithm scenario. Let us look at the problem which is very well called very well known as a scheduling problem. There is a single machine and there are N jobs. Each of these jobs can run only on this single machine and should be run on the single machine. And the jobs are already known to have running times t1, t2, tn respectively. In other words, the job J i takes T i units of time on the machine M. The aim is the following, the aim is to identify a schedule of jobs that is the order in which the jobs will execute in the machine so that we minimize the sum of the completion times. Now minimizing the sum of completion times can also be thought of as minimizing the average of all the completion times. It is very important for the student to note that this average duration, this is not the average duration which is a fixed number. It is the average time that a machine spends waiting for the average time a job spends waiting for a machine and then the time that it spends on the machine itself. One wants to minimize this quantity. Therefore the goal of this exercise is to find an ordering of jobs to execute on the machine so that we minimize the sum of completion times. Let us do this small example. There are four jobs. Job 1 takes 15 units of time. Job 2 takes 8 units of time. Job 3 takes 3 units of time. Job 4 takes 10 units of time. The total duration that the machine will spend executing these jobs clearly is 15 plus 8 23 units plus 3 26 plus 10 36 units of time. Therefore the average duration on the machine seems to be 9 units of time. Is 9 units of time. However, the completion time is a completely different entity as shown by this graphic. What are the first schedule in this graphic where job 1, job 2, job 3 and job 4 are scheduled? Job 1 finishes after 15 units of time on the machine. Job 2 then finishes after 8 units of time. Therefore the completion time of job 2 is 23. Job 3 which actually spends the least amount of time on the machine which is 3 units of time completes at time in time in time 26 and job 4 completes at 36 as a time instant. Therefore now if we average these completion times we see that the average is 25. That is 100 units of time is spent. The sum of the completion time is 100. On the other hand let us consider the second schedule where interestingly the shortest job is scheduled first. Then the second shortest job then the third shortest job then the fourth shortest job which is the order. Job 3, job 2, job 4 and job 1. Now observe that the average completion time is 17.75 units of time which is smaller significantly than the schedule where the jobs were scheduled according to the order in which they were presented in the input. So now do we have an algorithm here to minimize the sum of completion times? To do this let us write down a formula for the sum of the completion times. Before that let us look at the schedule. Let us look at properties of the schedule. Before we do the calculation for the sum of the completion times let us look at the property of a schedule. One observation that we can make is that if we take an arbitrary schedule and exchange the position of a shortest job in particular let us say the first shortest job in the schedule with the first job in the schedule. So let us look at our example in the first job. If we exchange the jobs J3 with the first job observe that we will get a better schedule than the one that was given first. So if we assume that there is always an optimal schedule where the first job is the shortest job then it is clear that there is a very interesting optimal substructure that if you remove the shortest job the remaining schedule for the remaining jobs is indeed optimal. In other words if you take a optimal schedule in which the shortest job is schedule first by removing that shortest job the remaining schedule is indeed an optimum schedule for the remaining n minus 1 jobs. Of course we do not know if there is an optimal schedule which contains the shortest job first that is what we are going to study now by writing down a formula for the sum of the completion types. So do this let us assume that there is a schedule which we refer to using the Greek letter sigma and let us assume that sigma 1 denote the first job and sigma n denote the nth job in the sequence. Now let us look at the sum of the completion times the sum of the completion times is written as a first formula where there are n jobs and let us observe that the time taken by the first job that is the job sigma of 1 that will be counted it will delay every subsequent job. Apart from using T sigma of 1 units of time on the machine the first job that we schedule which is sigma of 1 takes T sigma of 1 units of time on the machine not only that it also delays the remaining n minus 1 jobs by T sigma of 1 units of time same for T sigma of 2 it is a second job its completion time is the time that it spent waiting for the machine which is T sigma of 1 plus the time that it spends processing on the machine which is T sigma of 2. In general if you look at the whole expression for the sum of completion times the formula is given there it is n terms in the summation and there are n minus k plus 1 copies of T sigma of k and that is explained in the equality there which says that the completion time is T sigma of 1 plus the completion time of the second job which is T sigma of 1 plus T sigma of 2 plus the completion time of the third job which is T sigma of 1 plus T sigma of 2 plus T sigma of 3 and so on up to the completion time of the nth job which is T sigma of 1 plus T sigma of 2 up to T sigma of n. If we relate this summation by adding and subtracting a few terms we get the third term in the whole equality sequence which is viewed as 2 summations observe the first summation which is n plus 1 multiplied by summation of the processing times of the n jobs that is the first term minus summation k times t sigma of k that is the kth job the processing time of the kth job observe the first term is independent of the schedule and the second term really is very dependent of the schedule observe that this sum of completion times is indeed valid for every schedule sigma this is a very important thing. So, what we have done in this slide is to write down a close form expression for the sum of completion times for arbitrary schedule which we have called sigma and then we have observed that this sum of completion times can be viewed as 2 summations 1 that is independent of the schedule itself and the second one which is dependent on the schedule and there is a subtraction term there therefore, as a second term increases the total cost becomes smaller. So, let us see what makes a second term to decrease let us see what makes a second term to actually increase and therefore, reduces sum of completion terms completion times. So, let us see a property of an optimal schedule. So, let us imagine a schedule sigma and let us assume that there is an index x which is more than index y but the processing time for the job which is scheduled as sigma of x is smaller than the processing time of the schedule job schedule sigma of y in other words y is scheduled sigma of y is later than sigma of x but the processing time of sigma of y is larger than the processing time of sigma of x. So, let us try the most natural thing let us exchange the positions of the two jobs. So, in other words what do we do we exchange the position of the job in position sigma of y with the position of the job sigma of x and let us see how it changes the duration. So, let us see what happens we should just go back to the formula and observe that the terms just change by modifying the multipliers appropriately. In other words we have written down an inequality which says that x times t of sigma of x plus y times t of sigma of y. This is the contribution to the second term by these two jobs and when x change the position of these two jobs the contribution just becomes y times t of sigma of x plus x times t of sigma of y. We now show that x times t of sigma of x plus y times t of sigma of y is smaller than the summation after swapping it that is y times t of sigma of x plus x times t of sigma of y. This is very easy to see by the following sequence of expressions by just rewriting x times t x times t of sigma of x plus y times t of sigma of y. This sequence of expressions actually shows that the exchange actually increases the value of the second term. As a consequence of this it is clear that if we start off with an arbitrary ordering and if we identify a pair of jobs at sigma of x and sigma of y with the property that sigma of y is later than sigma of x and the time taken by the job is scheduled at sigma of y is more than the time taken by the job scheduled at sigma of x. If we exchange these two jobs the contribution to the second term in the sum of completion times reduces and therefore we have a new schedule whose sum of completion times is strictly smaller. Consequently the greedy algorithm is a very simple algorithm it says that the from the given set of jobs schedule the shortest duration job first. That it is indeed an optimal algorithm follows from our analysis of the completion time that this schedule has a smaller completion time than any other schedule.