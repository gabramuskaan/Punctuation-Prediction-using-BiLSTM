 Good morning class. So, far we have learnt about various ADTs namely the stacks, queues, lists and trees and today we are going to learn about another kind of ADT which is very popular especially in practice and the objective of this ADT is primarily as we already have seen in all the ADTs the objective is to put a set of items into the ADT or the object and retrieve items from the object. And what is our objective in the process that whatever operations we perform in the ADT things are done very very fast. Whether it is an insertion, a deletion or modification this is what we have seen in the context of the mind research tree priority, queue, stack, list, queue whatever. So, now here today we are going to talk about a different type of an ADT where we keep the size of a table fixed and see if we can kind of retrieve the attributes of a particular item in the table in a very quickly. We also want to modify the attributes of that particular item on name and we want to insert new names with its attributes, deletion name and its attributes and all the speed like to see if we can how quickly can we do it. The difference here is unlike the stack or the tree or the queue or the list here is a ADT where the size of the table is kind of fixed but it gives us very good performance and we will see this. What we have seen depending upon the two retrieve items from a table you may say you know we could also use what we have already done the ideas can we do better and we have already seen whether it is in a binary search tree for example, if you want to retrieve an item it was an unbalanced search tree then the time complexity is as high as order of the height of the tree. Similarly in a list for example, if you want to locate a particular item again it is as large as the length of the list there are n items in the list the time complexity can be out there. What we are going to try and do is can we do better. The area that I am going to talk about talk about you might have already guessed it is what is called a hash table and we basically use a hash function to access an element from a table. There are two types of hashing static hashing and dynamic hashing I am going to talk about static hashing what is static hashing is all about is that the identifiers or names are stored in a fixed size table called the hash table. The address or location often identifier x is obtained by computing some arithmetic function h on x. So, h of x gives the address of x in the table the hash table is partitioned into what we call b buckets and each bucket can hold a set of s records. So, the total number of items that can be show stored in a hash table is s b and h of x is a function that maps the set of possible integers to possible items into integers between 0 and b minus 1. The key can be anything any kind of string for that matter. So, we have a table here we call this the hash table let me call h t this has indices which go from 0 1 to up to b minus 1 and our objective is and it can be such that let us say these are the indices each index can store a set of items. All right let me give you an example maybe little big what I am talking about over here h t is essentially an adding and it is consists of b buckets and the index of these buckets are 0 3 b minus 1 and each bucket can hold a set of identifiers. Why does this happen essentially that if you are depending upon the kind of function that we have x is the identifier name and if I pass the identifier as argument to this function h it is possible that to identify map to the same bucket. Haching is very effective and why is it effective because if you look at an identifier which is 6 characters long there are t you know 26 to the power of assuming that we are using the letters of the English alphabet we have 26 to 36 to the power of i distinct possible values for x but any application actually use a very small number fall fraction of these. That is what we are saying is there are so many different possibilities for a 6 character identifier which is given by the expression t in this view graph but out of which only we in a application you are going to use a very small fraction of them. So, identifier density is defined as if there are n identifiers and total t is a total number of possible identifiers then n by t is what is called the identifier density. Then if there are b buckets in the table and each bucket can hold a set of identifiers the loading factor alpha is total number of identifiers divided by the total number of items that can be held in the stable. For example, let us say I have bucket 0 to b minus 1 in this example let us say each bucket can hold 2 identifiers that is what we mean over here that is s here and b is the total number of buckets and that is what you can hold. We define what are called synonyms if h of i1 i1 is a particular key value i2 is another key value identifier value name value whatever may be I am using all of these interchangeably here then we say that 2 identifiers are synonymous if both of them hash to the same value. Then we also define what is called an overflow it is possible that a new identifier i it has only 2 slots suppose it has to this then we do not have a space to keep it here because each bucket can only hold 2 items in this example then we define overflow when an identifier has to do a full bucket then we call it a collision when 2 not an identical identifiers has to the same value which is almost like a synonym here and how do we handle the problem. So here is an example we have a hash table with 26 buckets and 2 slots per bucket notice the indices over here the indices go from 0 to 25 and what I am doing is let us say I am using the first letter of the identifier to put it into the particular bucket and what I used the each slot for example is defined by the first letter in the given identifier or key value. So 0 goes into slot into the bucket 0, B goes into the bucket 0 B goes into the bucket 1, C goes into the bucket 2 and so on and Z goes into the bucket 25 and each bucket can hold 2 identifiers. So here I have a and a 2 and here in d I have only 1 element g a and g are 2 identifiers which are stored in the 6 slot and the notice that there are lot of empty slots. This is one thing about a hash table that you can have as we said identifier the number of identifiers which can be stored in R stored in a hash table and a total size of the hash table notice that we only have 5 identifiers but we have 26 into 2 slots therefore we identified density the loading factor on this hash table is very small. Now obviously what would we like X is an identifier let us say you know some variable name called let us say I have a variable X non 0 or something like that. Okay a variable name like this and this has to be or an identifier name which has to be hashed into this then I also have let us say Y non 0 and so on. I am using 7 characters in this particular example. Now what we want I can have a huge hash table what we would like is and so I am on this H of X and H of operating with the function hash function H of X on it. So H operates on this string operates on this string and gives me an index this let us say gives me I 1 and this gives me I 2. What would be ideally like if the 2 identifiers in this case for example X non 0 and Y non 0 are different okay different by 1 character right. What we would like is the hash function should be such that it spreads these keys as much as possible on the table. Why so if X non 0 and Y non 0 hash to the same index there will be a collision and what am I going to do. So maybe I will put X non suppose it hashed here 0 and I put Y non 0 after this. So next time if I want to retrieve this particular item once again I am giving Y non 0 or X non 0 and it will come to this and then to access Y non 0 I have to go through this bucket and look at the second element in the bucket. So what we would like is if I the keys are distributed as uniformly as possible on this hash table the performance of the hash table can be very very good okay. So crucial part in this hash table definition is what is called hash function which transforms or identifier or a name X into its bucket address. There are number of different techniques and if you look at Kanut's book on the art of programming he has a whole lot of hash functions that he has defined. Midsquare for example square the identifier that means you converting it to the integer square the identifier then use an appropriate number of bits from the middle to obtain the bucket address this is one division divide identifier that is compute the remainder and access divided by on this is the most popular and that you will also see in the example problem that is given and use this as the hash to hash. Folding is partition identifier into different parts add partitions and use the status. So number of different hash functions that people have come up with to ensure that the keys in a given application are distributed as uniformly as possible. Another variant which is not part of the beyond the scope of this particular course is what is called dynamic hashing. As number of items to be stored increases the performance of static hashing because the size is small deteriorate showing to collision the hash table must be extendable dynamically which I do not want to talk about in this course but you should know about it okay. So let us do an analysis of hashing how good is a hash function. So I am going to take a particular hash function assume that there are k items and the table sizes n then the probability of hitting a free location the first time is 1 minus k over n okay because there are already k items and I want to choose one of the locations where the k items are not present. So in general what we can do is the probability PI of an insertion requiring I probes assuming that the collision handling mechanism what it does is let us say the first time you when you hash this when you ran h of x for x of 0 and y of non 0 it came to the same location. Then hopefully the next time when I make a probe and let us say this already full it will distribute it says that this location is not used and it accesses the other locations then we have that so basically what do we have here probabilities n minus 1 minus k over n that is what we saw. The second time it is collided with this k by n and then there are we assume that it is not using any of them say n minus k by n minus 1 one of these it is collided so it is choosing the rest of the locations and p 3 for example k by n k minus 1 n minus k and so the third probe the first probe the second probe the third probe that it hits a free location and we can define this as for the I th probe. Now let us compute the expectation of this of the k plus 1 of expectation is basically I p I and the table size is n and the number of keys in the table is n then we can find you overall average number of probes that are required to insert an element number of probes that are required to insert an element search for an element whatever it may be there is small expression over here which says how you get this complexity and it turns out this value of E the expected number of probes is minus 1 over alpha log of 1 minus alpha what is alpha now alpha is the total number of m is the total number of keys which are present divided by n plus 1 and it is called the loading factor of the hash table here is an example alpha is let us say the loading factor notice that point 1 means you have 10 percent full point 2 5 means 25 percent full 0.5 is 50 percent full 0.75 is 75 percent full 95 percent full 99 percent full and so on. What do we mean by that where I look at this particular table I have only 5 elements out of 52 here so that is the percentage of items that have been stored in the hash table what is it giving here is that it is telling me that if the hash table is having only about 10 percent of identifiers which are present 25 percent of the identifiers that are present this is giving me the expected number of probes and what is very interesting to see is that even if the hash table is 95 percent full if you have a good function such that it redistributes the keys against such that it not using that particular location then we can see that the hash function gives for example even when the hash function if the table is 99 percent full it only requires 4.66 probes on the average or 4 5 probes on the average I wanted to compare this with a binary search tree a balanced binary search tree with a large number of items. So, it basically it is going to depend upon even a few assume a balanced binary search tree and if the number of items is very large then the depth is divided defined by log in. So, let us see I have 1024 items and then I am going to have 10 steps to identify me to determine the location of that particular element what is nice about the hash table is notice that it does not talk about the number of elements it only talks about the loading factor what is the loading factor this is the size of the table how many items are stored in the table a ratio of the number of items that are stored in the table to the size of the table ok. Now, this is for a very good hash function another hash which I just showed you here this is called kind of linear probing what we do is it has to the same value and we probe at that particular location let us say these are consecutive locations and we search for the particular element. So, for example, if it has to the same value x drawn 0 is not what I wanted y non 0 is not is what I want. So, it access this item and it was not it and it has to do one more probe to find out what the element is this is called linear probing and if we did this then there are two ways of doing this in a hash table for example, so let us say 0 through let us say I have a whole huge hash table like this whatever it may be of size s b then one kind of hashing scheme let us say these are the this is the location at which it hashes to let us say. So, it can be any number of elements 0 through s b minus 1 here then what we do is we run the hash function it let us say maps to one of these then what you do is suppose is already filled then you find the next nearest empty location and put the element ok this is actually called linear probing this one we are doing it. Now, the one is you divide the hash table into buckets like we have done here to b minus 1 and at each location have what is called a linked list ok. So, this is let us say the first element in this example let us say here this is where we saw it. So, let us say this was x non 0 and at its attributes at then this is going to point to y non 0 ok and so on. So, basically what we are saying is we have an array of lists over here this list need not necessarily be implemented using a linked list it can also be implemented using an array which we have already seen. So, you have hash table which is an array of lists and whenever there is a collision it is going to keep on adding a new element to this list. This is a different this is one type of hashing is also using linear probing here in the other hand what you do is you have one long table and on this long table if the first if a location is already occupied then you linearly go down find out what is the empty location and put it there. So, when you are searching for an element what do you do you search for that particular element it hashes to this and then from there you linearly search and find out where the element is ok. So, you can do various types of hash functions so the crucial properties on designing the hash function and what is interesting is suppose we use linear hashing there is a proof which says that if alpha is the loading factor then expected number of probes is this. I wanted to look at this table this is very very exciting in the sense that again here 10 percent 25 percent 50 percent 75 percent 90 percent and so on and you find that it actually requires only about 10 probes even when the hash table is about 99 percent full and remember 99 percent full does not mean that your the that the number of elements is fixed can have any number of elements but the identifiers number of identifiers that are stored in that table the loading factor should can be even up to that much. So, there is something about hashing hashing is a very very powerful technique for storage and retrieval of keys it has been extensively used primarily because of its speed and especially if the size of the table can be adjusted. There are algorithms today for re adjusting the size of the table well table become 75 percent full you double the size of the table redistribute the keys with a again you need a good hash function which will redistribute it uniformly and you can show that the amortized analysis of that is just about order one. So, this is something which is very exciting about hash tables the primary issue with hashing is deletion it is a little combustion because let us say I found this item here I did not find my item I have to continue to linearly probe and once I delete the item there might be an empty slot and we have to worry about this how do you handle this. So, what we have talked about is a special type of an abstract data type and this is called a symbol table ADT and it is very popular where you have strings which get mapped into locations and each key for example, can have a whole lot of attributes which can be very quickly unwritten in fact, you can have a hash table of objects it is very common hash table of function pointers the very common to see in practice which makes these speeds of algorithms very very good very efficient especially in practical applications. So, we will stop here this completes the lecture on hash tables.