 We did a recap just a while ago and now, we will look at the efficiency of the programs that we have done. And as I said, you could have the same problem being solved using multiple algorithms or multiple programs. So, which one is better than the other? That is what we want to find out. Let us take a simple example. Let us say I want to find whether it given non-zero positive integers prime or composite. All if you know what a prime number is, what a composite number is, a prime number is a number that is divisible by another than one and itself. So, others numbers are all called composite numbers. One is the only exception is called trivial divisor. So, what will you do? Look at the definition. It says a number that is divisible by non-other than one and itself. So, I can start with a simple program. See what we have here? Let us say n is the given number. We start with a variable i which is initialized to 2. We initialize a flag e prime to 1 and then if n is not equal to 2, then what are we doing? We are checking whether n is divisible by i or not. And if it is divisible by i, then the reset is prime to 0. Otherwise, we increment i to the next value and keep doing this. And finally, if not is prime, then you say number is composite. Otherwise, you print that the given number is prime. This is a small segment of c code. Now, the question is, do we really need to run this prime classification program? So, if you check for every value of i, can we do better than this? So, for example, any number. So, what will mean a number is prime? A number is prime means, if I take example 13, the factors of 13 are only 13 and itself. On the other hand, if I take a number like 4, its factors are 2 1 2. If I take a number like 25, its factors are 5 1 5. Let us take 18, its factors are 6 and 3. And let us take 24, its factors are 2 and 12. What is interesting is, here numbers here which are perfect squares, 4 25 and then you have numbers which are not perfect squares. But what we observe here? Clearly, if I take the square root of 18, the square root of 18 should be somewhere slightly greater than 4, because the square root of 16 is 4 into 4. So, it will be slightly greater than 4. What we see in the factors is, there is one number which is less than the square root of 18 here. Similarly, here you find one factor which is smaller than the square root of 24. So, what it tells is, I do not have to check for all the numbers going from 2 to n minus 1. All that we need to do is check up to the integer square root of the number. And if the number is divisible by any of the numbers, after the integer square root, then I do not have to test beyond that. Now, I think the P also notices, if a number is divisible by 2, what does it mean? It is a even number. So, for example, so what we can do is, we can illuminate 24, 68 from the division. So, what we can do is, we can start from 3 and then increment test with 3 within the numbers divisible, 57 up to the integer square root of the given number and that is the piece of code that I have written over here. What we are doing again, we are again setting e is prime of flag 2 1. And if number is greater than 2 and number percent 2 equal to 0, that means, if it is an even number, then e is prime is not 2 and then you just come out of the program. Otherwise, what you do is start with a divisor equal to 3 and keep checking whether the given number here for example, a number divisor 3, a number percent divisor equal to 0, then you reset the e is prime flag. So, what is it the piece on? We look at the prime classification program that we looked at, we looked at two versions of it, there are even much better algorithms for prime classification. And you find that one program clearly the number of steps that are required to execute the second program is much smaller than the number of steps that are required for executing the first program. So, what do we need now? You have to find out what is the running time of the program. So, how do we define running time? Running time clearly can be dependent upon the input to the program, quality of the code that is generated by the compiler used to create the object code. It can also depend on the nips of the machine, what do we mean by the nips of the machine? The faster the machine the quicker your program is going to run. But what we want to do is what the compiler generates, we cannot be sure whether it is optimized very much. And nips of a machine today I run my program on a Pentium, tomorrow I am running on a quad core or whatever. So, clearly what we need is we need to define this complexity in what is called a machine independent pain. Gendelies so, what we do is we define the size of the input rather than the input itself. For example, sorting a list of n elements depends upon the number of elements given in the list. What I mean is if I have an array to be socket, let us say I have an array with these numbers let us say I have two four ten fifteen ten ten ten ten sixteen twelve to be sorted. Suppose I have something like this to be sorted and I have another array with numbers let us say which are very large I have let us say twenty thousand ten twenty five thousand and so on. What we mean is the size of the number does not matter but as long as what is the dependent upon here? Both arrays are of size eight over here and that is what will matter when you looking at the computation of the running time of a sorting algorithm. Gendelies for number theoretic problems we talk about complexity in size three input and the size of the input is actually defined by the number of digit it takes to write number. For example, if I want to determine whether this number is prime or not the number of digits is five. So, complexity is defined by the length of the digit sequence that make up the given number. So, let us formally define what is the running time of the algorithm. Running time of the algorithm is given by this equation let us say the algorithm takes is input some size n then the complexity is let us say T of n equal to C in square that means it is it is quadratic in the number of elements or the size of the input that is given to the particular program. What is C here? C is a constant and units of T of n are generally left unspecified we talk about T of n in machine independent way and we are talking about operations that can be performed on an ideal computer. We will see in a while what we mean by operations on an ideal computer and generally when we are talking about T of n the running time of a program. We refer to the worst case running time that is maximum on all kinds of inputs. There are other types of time complexities which are beyond the scope of this particular course called the average of n which is the average over all inputs of size n and the average n of course, is computed assuming that all inputs are equally likely but there are other algorithms which talk about inputs which are not necessarily equally likely and the running time of such programs are also computed. But we will talk about very simple worst case time complexity. What you have to know is there are two specific type of notations one is called the big o notation and the big omega notations when we talk about time complexity of programs. What do you mean T of n when it is written as big o 1 squared it means that we can find two constants C and n not such that when n is greater than or equal to n not then and for a particular value of C T of n is guaranteed to run in less than C n squared time. Then what it is basically telling us is that if I have the input to the program in terms of a particular value of n then if I write T of n is equal to order of f of n then f of n is an upper bound on the growth rate. So, what how do you define this? We define time complexity like this let us say this is T of n as a function of n. So, what is it saying let us say now the actual running time of a program was let us say 3 n squared plus 2 n plus 1. I would write that this is equal to T n is guaranteed to be less than or equal to big o of n squared which means this is going to be less than or equal to C times n squared for a value of C and n which is greater than or equal to some n not. So, let us take this example let us take n equal to 1 then T of n equal to 6. Let us take a value of n equal to 1 and then T of n equal to 6. Now, let us say that I take C equal to 1 then what is it that we get when I look at T of n in terms of n squared here then T of n should be less than or equal to C into n squared and 1 into n squared. So, what for what value of n will it get set? So, now notice that I have taken C equal to 1. So, if n equal to 1 then what do we get we get that T of n is greater than this take n equal to 2 n equal to 3 n equal to 4 and so on and you can ultimately and take a different value of C for example, suppose I have taken C is equal to 6 for that matter. Then I would have got this one and this T of n is less than or equal to 6 n squared for n greater than or equal to 1. So, basically what we are saying is now what happens for n equal to 2 3 so on. So, what happens I can as n increase what is happening is this is going to be for n equal to 1 for n equal to 2 what happens we have T of n is equal to 3 2 is a 6 2 is a 12 plus 4 plus 4 plus 1. So, what happens when when n equal to 2 I get 6 into 2 into 2 12 into 2 24 16 17 therefore, it is still less than n squared. So, basically what we are saying is I have some T of n like this and then the big of n squared is going to be above this particular line that is the meaning of this big o notation and what is big omega say it says the big omega is the lower bound. If I say big omega n squared for example, if C equal to 1 we can always say T of n is going to be greater than or equal to n squared. So, you have a lower upper bound for the running time of the algorithm and a lower bound for the running time of the algorithm. What it says is T of n which is the actual running time of the algorithm is going to be bounded on one side by big o of n squared and omega of n squared on the lower end. There are certain things that you must worry about you must remember that when you look at big o of n what is the meaning of it it is a straight line. Now, you look at big o of n squared it is like this suppose I say big o of 2 to the power of n then what is going to happen it is going to go even more steeply this is called exponential running time exponential in n and when you have order of n squared order of n cubed and so on this is polynomial time and this one when it is big o of n it is linear time you also have one more big o of n for example, for all values of n if it takes order 1 what is the meaning of it is taking a constant amount of time whatever be the value of n that is a linear another complexity. So, what have you see basic idea of big o of n because we are talking about worst case complexity will not deal with so much omega n omega n, but we will look at big o of n. So, as a function of n we see what the time complex to you the problem is then n for example, increases and time taken does not change then we call this constant time big o of 1 when it is linear is a straight line then there is for example, n squared n cube or all polynomial and then for example, it is 2 to the power of n it is exponential for example, what do you mean by 2 to the power of n is very dangerous algorithm when n equal to 1 the time complex t is 2 n equal to 2 the time of complex t is 4 n equal to 3 the time complex t is 8 grows extremely fast. So, you should always worry about it whenever you write algorithms what we try to do is we try to see if you the more efficient the algorithm is for example, linear time is more efficient than polynomial time polynomial time is more efficient than exponential time and constant time is more efficient than any of these. So, those what we will like to look at there are other notations the theta n omega n small o f n small omega n and small o of n these are beyond the scope of this particular course, but enough to for you to know it. What it means for computing the running time of a program there are two specific rules the sum rule and a product rule the sum rule states that if a program is made up of two segments p 1 and p 2 what do you mean by this let me illustrate this what this means is that I have program p 1 here have another program p 2 here and I say then that this program p is made up of two sum of two program segments p 1 plus p 2. Remember we saw in the when we did the recap that you can you know you can use assignment statements make a big program alternative statements the big program combination of assignment alternative repetitive whatever. So, what we do is you compute a time complexity of p 1 separately compute a time complexity of p 2 separately and use the sum rule to compute the time for complexity of the program p. So, how do we do this if the program segment p 1 takes order f of n and program segment p 2 takes order g of n we are we know what order is g of n and f of n are functions of n then the total time complexity of the program is a maximum of f of n and g of n this is called the sum rule. So, what do we mean by this it let us say this program takes t of sum t of n is sum big of g of n f of n sorry and this program segment takes big of g of n then the time complexity t of n of the entire program is max of order of max of f of n comma g of n this is what it tells you ok to the C n example in a little while. Before that I would like to talk about another rule which is called a product rule product rule is p 1 programs as obvious p is made up of p 1 into p 2 what are the meaning of this what this means is that simply that you have some kind of a repetition block that is there. So, I have some kind of a loop statement here and within this this loop statement corresponds to let us say p 1 and within this I have the volume the program p 2 and basically it means that the program segment p 2 is getting executed p 1 times ok. Then what the product rule says is that if the time complexity let us say for this one is t 1 of n and for this 2 is 2 to f n then what the product rule states is the time complexity t of n is big of sorry t of n equal to f of n and let us say this is g of n then this is this is equal to order of f of n into g of n the product of the 2 time complexities. So, let us see an example now I have a small program segment again here some is initialized to 0 there is a fault for loop which sums the first n number starting from 0. So, now some is equal to 0 it is an assignment statement it takes constant amount of time therefore, we say it is order 1 look at this for loop for I equal to 0 I less than n I plus plus. So, this is again an assignment this is a comparison and this is an increment. So, all of these are basically like some rule with 3 statements being executed therefore, it takes the maximum of order 1. Then again here there is some kind of assignment here I am not written the rest of it here so, it again costs you order 1. But very time complexity for the segment of each loop the slope on the other hand is being executed n times. So, if you look at the outer program p 1 here that is going to be executed n times therefore, it is order n. So, whatever we do now so, this kind complexity becomes so, for the segment of each loop is order 1 but time complexity for the entire for loop becomes order of n into 1 because n is the time complexity of this for loop because it is executed n times and the total time complexity of this algorithm is order of n. And before that we also had this what you call order 1 statement here. So, if you look at the time complexity of the entire program now this is the program segment p 1 and this is another program segment p 2 and it is a some program now you apply the some rule and you get that the time complexity for the program segment is order of next what we will do is we look at another kind of complexity analysis. So, simple programs we know how to do assignment statements we can evaluate and loops we know that when you evaluate a loop you have to find out how many times a loop is executed. Before we go to recursive functions I would like to leave you with this how long does this program take to execute suppose I have I is equal to 1 and I say while I less than n I is equal to 2 s plus 1 minus 1 minus 2 star I. What is the meaning of it? So, basically how many times will be execute normally students have this habit of saying when I less than n they say n times that is not correct for example, let us take n equal to 8. So, I started out with I and let us see how many times this loop this body of the loop gets executed and let us take the value of I and n here I equal to 1 n is 8 then I becomes 2 n is still 8 of course, then I becomes 4 n is 8 I becomes 8 and n becomes n is still 8 and what happens I is not less than n and you execute you exit. Suppose n becomes n is equal to 16 then what will happen will have 8 to 16 over here and this will also be 16. So, once mode it will execute. So, basically what we are saying is depending upon the value of n here. So, notice that it is only from when you change from n equal to 8 to n equal to 16 it only executes the body of the while block one more time when n equal to 32 I am sure you got it right it will execute one more time compared to n equal to 16. So, if you look at it in terms of time complexity it is approximately log n times not exactly it is order of log of n to the base 2. Notice that for example, first time when it executed 1 2 3 and then next time was 4 and so on. So, the time complexity is not the same as of this segment of course, is log n. So, this is something that you must remember that you have to look at the kind of computation being done with respect to the loop to decide how many times it gets executed. The program that I had given here clearly we had done i equal to i plus 1 in the previous case over here. Therefore, it executes the n times whereas, this program which I have shown you that is not execute n times. So, next thing that I want to look at is how do we analyze recursive programs. All if you have seen recursion in the C part of your course. So, what are we doing here n is equal to n 1 then value set 2 I am just finding this sum of elements in an array I am just writing a simple recursive function this statement is what is important this is a comparator here it cos order 1 order 1 here in the other hand is order 1 and the way you write it is because it is a recursion here it reduces the size of the problem which was originally n to n minus 1 and it is order 1 for the addition and the assignment plus t of n minus 1 because this function is going to be called again. So, what we do is when you do time complexity analysis and if you look at it is C plus t of n minus 1 some constant time if n is greater than 1 it is equal to 2 d if n equal to 1. So, in general I can write that t of n is I c plus t of n minus I if n is greater than I and then I equal to n minus 1 what happens C into n minus 1 plus t of 1 C into n minus 1 plus t and therefore, the time complexity of this algorithm is order n. So, there is something here is something that I want you to be a little careful about recursion is somewhat dangerous. So, this was a function that we wrote if I write it in some kind of algorithm type of statements and I say that you know function recursive and n this was a function of this kind if n equal to 1 then do something else we said return one legacy return n else we said else it was like recursive is equal to n plus recursive of n minus it is something like this if I was going to write it in pseudo code the program would have looked like this. So, this harmless statement and then you found that the time complexity was order n. Now, suppose I replace this n here and make this recursive of n minus 1 what will happen to this program let us see what will happen. This way to analyze this is to draw what is called a recursion B. So, I started off with the pop most recursive of n and what do I do I do I do I write it into. So, it is going to call recursive of n minus 1 at recursive of n minus 1 and this one in turn is going to call what recursive of n minus 2 recursive of n minus 2 and this recursive is also going to call recursive of n minus 2 and recursive of n minus 2. Now, what is the issue and then n minus 2 will call n minus 3 and so on let us start with recursive of n minus 8 then this is going to call 7 and 7 or maybe instead of 8 let me start with 4 it is easier 4 calls 3 and 3 3 calls 2 and 2 2 calls 1 1 and that is when it will terminate 2 calls 2 2 1 1 and finally terminate. So, now if you look at this computation which was order n originally now if you look at the what is n now n is 4. So, now if you look at the number of computation 1 2 3 4 5 6 7 ok. So, basically 7 times the recursive call is being made and therefore, this one is time complexity in terms of 7 is of the order of 2 to the power n and this is something you must be very careful when you write recursive programs ok. So, this is all constant time over here. So, it becomes order of the exponential in terms of 2 to the power of n. So, imagine if it if it was 8 here and what would I say 8 given it becomes 7 6 6 5. So, there are 4 5 here 5 5 5 5 and so on and you notice that the tree is little start growing very very large ok. So, you have to that best way in my opinion to analyze recursive programs this is simply draw this recursion tree and count the number of nodes that are there in the recursion tree and that will give you exactly the amount of time the program is going to take to execute like we saw in this case big of 2 to the power of n ok. So, on the other hand I can also have here the recursive is let us say the recursive of n by 2 plus recursive of n by 2 you guessed it right. So, what is happening now 4 is getting divided becomes 2 2 2 becomes 1 1 and we are done ok. So, how many times it is basically this 1 1 2 3 add into 2 perhaps. So, it is only order n the time time complexity of this algorithm. So, these are things that you must keep in mind when you analyze the time complexity of recursive functions. So, we will stop here and then we will take 2 algorithms for sorting and searching and we will analyze the complexity in little bit of detail ok.