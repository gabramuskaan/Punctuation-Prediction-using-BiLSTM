 The last two lectures we talked about recursion and we saw that several problems can be solved very easily and very elegantly using recursion and in the last lecture we saw how recursion actually works and what happens at runtime where recursive function executes and so on. So in today's lecture we will talk about how and when not to use recursion and we will see that in many cases recursion is not appropriate and also in many cases if we carelessly use recursion we can actually end up with extremely inefficient programs. We will also introduce some notation for analyzing the performance of programs which is very important to understand because our programs should not just work but we should also try to make them as efficient as possible. So as we have already seen recursive algorithms for many problems are much simpler and more natural than iterative solutions and we have seen number of examples in the last two lecture but when we discuss how recursive function works would recall that when a the number of function calls that happens is very large depending on the depth of recursion and each function call and return has a substantial overhead because return address and local variables etcetera have to be create space for local variables and return address has to be created etcetera the parameters have to be initialized and so on so forth and this overhead can become substantial in certain cases especially if the recursion is very deep. So and also in terms of usage of memory recursion is more expensive because we have to create a stack frame for all the simultaneous invocations of the recursive function. So there is a trade off between using recursion and writing an equivalent non recursive or non iterative function for solving the same problem. So the ideal solution is that where the problem is very simple and the non recursive solution can be implemented as well as understood easily then we should always prefer not using recursion because recursion as we have seen as a substantial overhead. However in certain cases the problem itself may be very difficult to solve or the solution may be very difficult to understand if recursion is not used for example the tower of an oe problem that is all last time and in such cases certainly we should use recursion but for problems such as factorial and g c d which can be easily solved and the solution can be easily understood even if it is not recursive solution recursion is not really appropriate for these kinds of problems. Now, see how careless uses of recursion can lead to extremely inefficient programs and while some certain days will also introduce analysis of programs for analyzing how much time a program takes to solve a given problem as a function of the size of the problem in the sense and to do all that let us consider the following problem as an example. You have to write a function to compute x to power n for given values of x and n x can be a floating point number or an integer it does not really matter but n is a non-negative integer that is n is an integer which is greater than equal to 0. Now, how do we compute x to power n? The naive simplest solution is start with 1 and keep multiplying the answer by x till you do it n times. So, start with 1 and multiply it by x and times. So, this obviously requires n multiplication and the time taken by the algorithm would obviously proportional to the number of times the multiplication operation is performed. Now, if we think a little bit about this problem we will find that we can actually do substantially better than n multiplication. So, let us see how. Consider the following facts which all of us know about exponentiation x to power 0 is 1 for any value of x. x to power 2 n is the same as x to power n squared and similarly x to power 2 n plus 1 is equal to x times x to power n squared and we can use these facts to very easily come up with a efficient recursive solution to this problem of exponentiation and to see what this algorithm will be we have to simply utilize these properties that we just saw. If the exponent value that is given to us that is the power that we have to raise x to if it happens to be even then what we can do is we can raise x to half that power and then simply square the result and if the power is odd then we have the power obtain the result square it and then multiply by x that is what is happening in this case and that can very easily lead to a recursive solution to this problem we will see the recursive solution in a little while. But let us try to analyze how many multiplication will be required if we do that. So, let us say we denote the number of multiplications needed to compute x to power n in the worst case as t n where t where t is a function of n clearly the number of multiplication required will be a function of n and we have to find what this function t n is. So, clearly you can see that t 0 is 0 because x to power 0 is 1 and we can compute that without using any multiplication and to compute t n what we need to do is compute t to power to compute x to power n what we need to compute is x to power n by 2 and then square it and if n happen to be even that is the answer and if n happen to be odd then we have to multiply it once again by x. So, how many multiplications are needed to obtain t to power n by 2 the time taken would the number of multiplication would be t of n by 2 we are using flow because n by 2 would give us only the integer component which will be the flow of n by 2 in general and then we have to square that that is one more multiplication and then possibly we have to multiply the result again by x. So, in the worst case we require 2 more multiplications and so t of n is t of n by 2 plus 2 for all values of n greater than equal to 1. So, now this function t has been expressed as a recurrence relation and we can actually easily solve this recurrence relation by keeping on expanding the terms t of n by 2 writing that in terms of t of n by 4 and so on so forth. So, let us do that exercise. So, for n equal to 1 and greater than equal to 1 as we just saw t n is equal to t n by 2 plus 2 and then in the next step t of n by 2 using the same relation we can write as t of n by 4 plus 2 and so that becomes t of n by 4 plus 2 times 2 and in the next step it becomes equal to t of n by 8 plus 3 times 2 and so on so forth. So, now in every step we are dividing further dividing n further by 2 and finally, this value n by 2 4 8 16 32 etcetera finally, this value will become 1 and in how many steps will it become 1. So, we are dividing by 2 for 1 2 for 2 2 2 for 3 and so on and so finally, write by 2 for log 2 times n then this value will become 1 and so the point is that in that expression we will get t 1 plus 2 how many times log n times the floor of log n times and so t 1 we of course, know to be again t n by the same relation is t 0 plus 2. So, we add to one more time so finally, the number of multiplication turns out to be 2 times the floor of log n plus 1. Note that this is a logarithmic function of n whereas, in the naive algorithm we have to perform n multiplication which is a linear function of n whereas, using this square and multiply algorithm we require only the number of multiplication required is only a multiple of the logarithm of n and naturally for large values of n the even 2 times logarithm of n is always going to be less than n. So, that is how we analyze the running time of an algorithm and in fact, so essentially what we need to do is to find the running time of the algorithm in its worst case because that is what we are usually interested in. So, the worst case time taken is express function of the input of the input or of the input size. So, in this case we have express t n which is the number of multiplication but really the running time of the algorithm would also be proportional to t n. Note that we are not really bothered with the actual running time of the algorithm on a particular machine on any given machine the actual running time of a particular program will depend on so many factors like how exactly the algorithm was implemented and what was the compiler used and how fast is the machine how much memory it has and so on. So, for number of factors which cannot really be analyzed very easily but this notation so this notation gives us a way of analyzing the running time without really worrying about these constants. So, that is a big old notation that we are going to talk about. So, the running time in the running time expression that we obtain using the kind of analysis that we just saw usually the multiplicative constants are ignored because they would vary from different machine to different machine. So, those are not important and we cannot predict those constants really and also the lower order terms are not so important for reasons that we will just see. So, in the in the previous example we saw that the running time is a constant times log n plus a constant. So, both the value of the constant and the additive constant both are really unimportant in the big old notation. So, the big old notation used to express the order of the time taken or in general of any function. So, in this particular example the time taken by the square and multiply algorithm in the big old notation is order log n which is written in this fashion big old and within brackets function log n and this informally means that the time taken is some constant times log n plus plus terms which are lower order than log n. Now, the big old notation is very significant because it abstracts away the constants and the lower order terms and it is important to understand that if the order of an algorithm is greater than that of another algorithm. So, for example, order n is greater than order log n then the second algorithm which has a smaller order will always be faster than the first algorithm which has a higher order for large enough input values regardless of the values of the constants and lower order terms involved. So, for example, an order log n algorithm will always be faster than an order n algorithm for a large enough value of n. So, what this means is that if you have an order log n algorithm in the running time of the algorithm is some constant times log n plus lower order terms and for the order n algorithm the running time is some constant times n plus lower order terms. Now, regardless of what these constants and the lower order terms are we can always find a value of n beyond which the time taken by the order log n algorithm will be less than the time taken by the order n algorithm. So, let us see this in terms let us see this by plotting some of these functions. So, this plot shows three curves the blue curve plots 1000 times log x. So, this is an order log x function note that the multiplicative constant is 1000 the pink curve is a linear function of x it is 100 times x notes that even though the constant is 10 times smaller in this case, but still the linear function that is a straight line always overtakes this log x curve and this will be two regardless of what the relative what the constants what the magnitude of these constants are. And the third curve we have plotted is a parabola which is 5 x square and again the multiplicative constant is very very small compared to the other and it will also always overtake the linear the straight line as well as the log x curve regardless of the relative magnitudes of the constants involved. And that is why you know when we talk about the running time of an algorithm or the complexity of an algorithm, we talk about really the asymptotic complexity as it is called where we talk about very large values of the input size. And for very large values of input size as as these curves show the actual values of the constants and lower order terms are not really important it is really the most dominating term or the greatest order term which is really important that is what the bigotation really represents. So, we have got an efficient algorithm for exponentiation, but suppose we implement this algorithm in recursive fashion as shown in this code. So, x and n are given as parameter and the function is if n is equal to 1 return 1 as if n is even then return x to power n by 2 into x to power n by 2. Otherwise n is odd and so we return x into x to power n by 2 into x to power n by 2. So, this correctly computes the value of x to power n, but what is really wrong with this implementation? What is wrong with this implementation is that we are computing the same value twice. So, if you look at for example, this statement we are computing x to power n by 2 once here and then we are again calling the same function recursively with the same parameter once again. So, we are recomputing x to power n by 2. So, this is obviously inefficient because we needed to compute this value just once and then we could have reused this value to multiply it with itself. So, what we have seen is that x to power n by 2 is being computed twice. Now, for each computation of x to power n by 2 will have to compute x to power n by 4 twice which means overall x to power n by 4 will be computed 4 times. Similarly, x to power n by 8 will get computed 8 times x to power n by 16 will get computed 16 times and so on so forth. So, clearly this is extremely inefficient. Let us see precisely how inefficient it is by trying to analyze the time taken by this particular implementation. So, let us denote the time taken to be T n for where n is the exponent. So, when n is 0 clear is the time taken is a constant some constant C 1 and for n greater than 0 T n is 2 times T of n by 2 because we are calling the function power x n by 2 twice plus a constant and so this constant would be the time taken to multiply x into the power of x to power n by 2 into x to power n by 2 and so on so forth. So, as I said these actual values of the constant C 1 and C 2 are not really important. So, what I have seen from the previous explanation in the previous explanation this factor of 2 was not present. But, before this factor 2 has come suddenly this the time taken by this algorithm has gone back from being a logarithmic function of x of n to a linear function of n. So, I will leave it as an exercise for you to show that T n is actually all the n. So, this is not really a good implementation of the square and multiply algorithm the algorithm is good but, we have not really implemented it properly we have not used recursion carefully enough and if you really want to get the benefit of the speed up that the square and multiply algorithm offers us then we have to make sure that we from within a function we make only one recursive call to the function and reuse the return value. So, that is quite easy to do. So, here what you are doing is we if n is even we call power again to compute x to power n by 2 and save the return value in a variable T and then multiply T by T to compute x to power n by 2 times x to power n by 2. And similarly if n is odd we compute we save x to power n by 2 in a variable T and then compute x times T times T. So, there are in this function still there are two calls to the power function. But, for a given value of n it is easy to see that either this call will be made or this call will be made both of them cannot be simultaneously made because clearly n can be either even or odd it can be both. Of course, we could have we could have called power somewhere outside this statement in both the cases at a common place and then then use that value in both these but that would not really has been an indifference. So, this particular implementation of the power function now actually correctly and efficiently implement square and multiply algorithm for exponentiation and it does give the speed of benefit and has running time of order log n. Let us now take another example to see how recursion should not be used. So, let us consider the computation of Fibonacci numbers. The Fibonacci series is defined as follows F of 0 is 0 F of 1 is 1 and for n greater than 1 F of n is F of n minus 1 plus F of n minus 2. So, essentially any term in the series is the sum of the last two terms. Now, suppose you want to write a function to compute the nth Fibonacci number that is to compute F of n. You can see that we can write a very simple iterative algorithm to compute F of n. All we have to do is we have to remember the last two terms in the series and keep adding the last two terms to obtain the next term and in the next iteration the newly obtained term and the last term because become the last two terms and we keep doing this till we have obtained F n. So, here is the implementation of the simple iterative algorithm that we discussed as now. Cur and prev are the last two terms that have been computed in the Fibonacci series and we always maintain i such that curve is F of i and trev is F of i minus 1. So, if n is equal to 0 we return 0 otherwise as long as n is greater than i that means we have not yet computed F of n because the last term that we computed was F of i. So, we add up the two terms and that becomes the new value of curve and the value of previous is the old value of curve which we saved in t and then assigned it later to previous and then we increment i because now one more term in the series has been computed and finally, when n becomes equal to i we return the value of curve because that will be the value of F of n. Now, so that is simple enough but looking at this looking at this recurrence relation one might be tempted to implement the algorithm recursively by just adding up the values of F of n minus 1 and F of n minus 2 and use recursion to compute these two values and we will just see that that would be extremely inefficient. But let us first analyze the iterative algorithm that we just wrote and see how much time it takes. So, note that each iteration of this loop takes a constant amount of time it does not really depend on the value of n and the number of iterations is n minus 1 and so therefore, the total time taken by this function is order n where n is the parameter supplied to the function. So, let us say we write that straightforward naive recursive function to implement Fibonacci. So, this is easy to write that it as it turns out it is extremely inefficient and we will see why it is inefficient. So, if n is equal to 0 or 1 we return the value of n because F of 0 is 0 and F of n is 1. Otherwise this function simply makes two recursive calls to itself compute Sibonacci of n minus 1 and Fibonacci of n minus 2 and returns there some. Now, why is this implementation really inefficient? This is inefficient because the same values are again being computed multiple times. F of n minus 1 is computed only one but F of n minus 2 is computed twice because when F of n is called it calls F of n minus 1 and F of n minus 2 and F of n minus 1 again calls F of n minus 2 and F of n minus 3. So, if we try to see how the recursive calls to F are happening see that F of n calls F of n minus 1 and F of n minus 2 and this calls again F of n minus 2 and F of n minus 3 and this calls F of n minus 3 and F of n minus 4. So, see that F of n minus 2 is being called twice and F of n minus 3 will actually get called 3 times because F of n minus 2 will again call F of n minus 3. So, F of n minus 3 is called once here once here and once here and if you look at F of n minus 4 that will be actually called 5 times because it will be called from here and it will be called from here and from here and from here and so as the value of I increases F of n minus I is called an increasing number of times. So, this is obviously grossly inefficient and if we write a recurrence relation to understand the time taken by this implementation we will find that for n equal to 0 and n equal to 1 clearly the time taken is a constant but for n greater than 1 T n is T of n minus 1 plus T of n minus 2 plus a constant will not try to mathematically solve this second installation but it can be shown that T n T of n is actually order a constant time constant to trace to the power of n. So, what that means is that the time taken by this algorithm by this implementation of the Shibunachi series completion the time taken grows exponentially as a function of n rather than as a linear function of n as in the case of the simple iterative solution. So, that is extremely inefficient. This is the end of today's lecture in the next lecture we will talk about a couple of very important and frequently encountered problems these are the problems of searching and sorting and we will try to develop some simple algorithms to solve these problems and try to analyze these algorithms to find their time complexity. Thank you.