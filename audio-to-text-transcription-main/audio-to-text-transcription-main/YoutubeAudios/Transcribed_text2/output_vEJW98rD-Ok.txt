 We were talking about errors and round of errors and percentage errors. Now, I will briefly show you that if we commit an error, how that error continuously gets accumulated and ultimately has a much larger effect. For example, suppose the exact value, suppose the exact value of some variable at the first instance is 10. Suppose, no, suppose the exact value is 9.98 and we compute 10. So, the inherent error first the error actually is 0.02. Now, as we go on iterating, suppose x 1 is 10 times x 0. Then the exact value should be 99.8, but here we will get 100. Suppose it is 100. So, you can see that the error has increased to how much 0.2. Now, if the next iteration x 2 is again 10 times x 1, then it will be 998, whereas the computed value will be 1000. So, the error is becoming 2. So, you see how with if we start with an inherent error, how the error accumulates over time. So, we can see a very important term is relative accumulation of error, which is accumulated error, accumulated error divided by exact value for that iteration. For example, in the first iteration, it was the error accumulated error was 0.02 divided by the exact value, which was 9.98, but later on it became 0.2 divided by 99.8. So, this was 0.00 but whatever that is, that is not that important, I do not want to confuse with this. Now, there are some cases where this accumulation of error actually goes on increasing. If the rate of accumulation error decreases, if the rate of accumulation of error decreases or if the rate of accumulator increases, but the rate of relative error decreases, then we call it a stable algorithm. However, I am not going into the details and the intricacies of this, but this is just to give you an idea how the error propagates through iterations. And so, we must be very conscious about the rate of increase of this error, right. With these words, we move to the algorithm, which we are planning to discuss in this lecture, that is a bisection method. I have already told you that the bisection method is given a particular function, some function on this x and y axis. If I have some function that moves in this way, then we start with any two points, any two points arbitrary points here and maybe here. No, these two points will not do, because if I select these two points, then both of them are positive, so that if I had selected this point, for example, these two points x 0 and x 1, that would not have served my purpose, because I do not know whether the root is there or not. So, I would rather select two points, which are of opposite signs and therefore, I know that somewhere in between the root lies. So, now I will take the midpoint of this somehow here. If this point is negative, then I will keep the positive fixed and I will find out the value of y, so these two are opposite signs, so the root must be somewhere here. So, in that way I come to this one and find out the value of the root here, in that way I go on dividing it till I come very close to the root, as is being shown here, very close to the root. Now, how close that will depend on my decision, that is the basic approach of bisection method. So, with that, let us try to have a look at the algorithm. Therefore, first we start, we define the function, we define the function f x and get the value of the interval a b, that is there is a function. So, here there is a function and the function can be long enough. So, I take the limits that I have to find out the root within this interval a and b and I find out how much error is required. So, how much error is acceptable? So, this error is the allowed error is the epsilon that I was talking of and also the number of iterations, the number of iterations because it may be that in some case, I am not finding the root because I am going on looking at say for example, this sort of scenario and my a is here, my b is here, then obviously, the root does not lie between this. So, I will go on doing this bisection and again doing this bisection, how long will I go on, but still I am not I will not find I may it may be that I will not find the root. Therefore, there is a maximum limit that is kept number of iterations. Now, I initialize I to be 1 some I just some index to be 1 and then I call a sub root in or a function bisect the midpoint that means. So, here is a sub sub root in bisect, what it does here is a sub root you see what it does or a function what does it do here, it finds out between a and b the midpoint and increments the iteration and pins the value of x 1, x 1 is the middle point. So, if my function was like this and this was a, this was b, then the root must lie somewhere here, I find out the midpoint of this. So, this becomes x 1 to be there is a next one and how many what is the iteration. Next is f that means, f of mid less than 0. So, there can be two things, all right here I select this to be my b and this to be my a. Now, obviously, if I take the midpoint then the midpoint will be somewhere here a and b were of different signs. If f mid is not less than 0, then a should be x that means, I will now move it on this side and try to find out this should be this should be the next a and between these two I will have to find out and every time I am trying to find out whatever value of f x that I compute is it less than the absolute error. If yes, then x that is I am getting my solution otherwise I am going on doing this. So, this is a flow chart of the whole thing, but I think you will be more interested in looking at the algorithm and let us look at the algorithm for a second and the program here is the algorithm you see this is much more understandable to you. I start I read x 1 x 2 and the error here x 1 and x 2 are the initial cases all right. Here is my thing I have taken this is x 2 this is x 1 is the absolute error that means, how much error is permissible compute f 1 that is f x 1 compute for this function this value and f 2 compute this value all right. If f 1 and f 2 the product of these two is greater than 0 greater than 0 that means, my initial cases are wrong because both of them are positive. Then I can do many things I will instead of going to 11 my initial cases are wrong I will again ask for new guess all right. So, I take a new guess and I find that that is less than it less than 0. Then I take in this step x 1 plus x 2 meet of that. So, suppose meet of that is this one and that is becoming x if x 1 minus x 2 by x actually here it should be if I think it is wrong here it should be if f x 1 minus f x 2 please read this as f x 1 no the error f x 1 minus f x 2 divided by x is less than e then display x that means, if my error between these two between these two points the val difference is less is 0.002 and that is I just over that I can assume as 0 then I will display this particular value of x. So, right now it is not the case otherwise I will make this f to be f x I take this all right. Now, between these two I again divide I come here and in this way I go on all right. So, you will be able to write the program as the program runs. So, just to show you I am sure you can write the program yourself how can you translate this in the form of a code. So, here you see let us try to understand this code it is a C program for the bi-section method I have included STI O dot H math dot H and there is some function because I have to find the root of a particular polynomial. So, for example, here it is given this is a polynomial. So, what is this polynomial it is x cube minus 4 x minus 9. So, that value of f x has to be continue computed. So, the function is the fun fun is the name of the function then there is another function bi-section this function performs and brings the result of one iteration. So, it is a plus b by 2 now in an earlier lecture we had talked about this what is this because here when I am calling this function I am calling by reference how I am calling I am just passing the address x and whatever I do here once again is a part of revision you can see if I come here flow star x that means what x is there some variable and I have just passed the address of that. So, star x is the content of this say 50 all right. So, I passed it on and in the main function I am passing on the address of x this we have seen earlier. So, I am taking the meat point and incrementing the iteration iteration is also a call by reference and flow tape flow b are two points in between which have been passed on. Now, what is being done in the main function in the main function I am setting the iteration to be 0 and here I am saying how many iterations are permitted maximum number of iterations x a b allowed error how much error is allowed and some x 1 value is given enter the values of a b allowed error and maximum iterations. So, all these I read the range a and b between which points I have to do a and b and how much is the allowed error and what is the maximum number of iteration then with this I call by section what do I do by section and x that means this will be give me the meat point I will call by section here a b iteration. So, a is being passed here b is being passed here and the number of iterations is being passed here. Now, here I am finding the meat point and that meat point is being returned here is common right and then here at this point I find out I call the function that is I am computing the polynomial if the polynomial at a value of the polynomial at a and the value of the polynomial at x that is the meat point is less than 0. So, what happened this was my scenario here was a and here was b no let me draw it in there. So, here was b and here was a. So, now what I do I got the meat point somewhere here then the value of the function at this point and the value of the function at this point negative therefore, I move this b to x x is becoming b and I do the same thing otherwise if it was on the other side I would have made x to be a this clear and then again I call by section after calling by section I find if the absolute error of x 1 minus x is less than the allowed error then I will print the loot. So, this will go on while I did this is under this do while this will go on until I exceed the maximum iteration. So, this is how I code and all of you should be able to practice this yourself. Next we will move to another another algorithm another algorithm which is another method which is known as the Newton-Raphson method. This method adopts a different approach to find the root of the function let us try to understand this briefly. So, I have got a function like this now Newton-Raphson method what it does at it starts at some x 0 and the corresponding value of the function at x 0 is f x 0. Now, what it does it finds out the tangent at this point tangent to this point. So, what would the tangent b the tangent is nothing but f prime x 0 because we know that is f prime means d f x d y d x. So, I draw the tangent here the tangent intersects the x axis at some point. Now, I take the x 1 this let this be the value x 1 I drew the tangent and got the value x 1 ok. I come here and from here since the value here f x 1 at every stage I have to check whether the value is close to 0 or not obviously, this is not the case. Therefore, I draw another tangent from here and what is this tangent this tangent is f dashed x 1 derivative at this point. So, this becomes x 2 now I find out f x 2. Now, again compare whether f x 2 is very close to 0 or not still it is not the case. So, I draw a tangent from here to this I am sorry I should have made it dotted as I was doing with all I draw a tangent at this point. So, this is x 3 and I find out f x 3. Suppose, this value f x 3 is very close to 0 suppose this is within my allowed error then x 3 is the root. Otherwise, if it was not there from here again I would have to draw a tangent in this way it goes all right. So, this is the essence of Newton-Raphson's method. So, what are you doing here? We are taking a function starting with a point and finding a tangent to that curve to the function at that point and see where that tangent intersects the x axis. From there I find out f x 2 and then I go on doing this. So, I think this geometric exposition will be very helpful to you. So, next let us try to see how Newton-Raphson method works. So, at every stage x n plus 1 is x n minus f x n by f dashed x n. Why? Because of the simple reason that I had this curve and this was my x n all right. From there I drew the tangent. So, this was f x n and I drew the tangent here then if I divide this and subtract it from here, I will get this x n plus 1 here. So, you see it is coming in the other way all right. Because at every stage I am computing this next. Here x n is the current known value of x. f x n represents the value of the function. f dashed x n is the derivative of the slope at that point x n plus 1 represents the next x value that you are trying to find. So, this expression is coming from the fact that f x by dx where dx is delta minus x. Therefore, the term f x by f dashed x is actually the value of dx. How much the dx value sorry dx value? That means, how much I should come down. So, you can see from this expression f x by f prime x is f x by f x by delta x that means, the delta x actually this is the delta x part and that means, how I am shifting this x. So, the x was here and I am shifting it by delta x and coming here again shifting it by delta x coming here like that I am going. All right. So, suppose f x let us take an example here. Suppose f x was x square minus 4. Then f prime x is obviously 2 i x and x 0 was 6. Suppose, I assumed x 0 to be 6. So, here is how it goes. First iteration x 0 is 6 f x 0 here, f x 0 is suppose 32, 32 because 6 square 36 minus 4. If prime x is what? If prime x is 2 x that is 12. If prime x is 12, then x plus 1, sorry I am sorry this x n and the next value will be x n that means, 6 minus 32 by 12. So, whatever that is I subtract and I get the x 3.33. Next iteration I come to 3.33. So, it was something like this that I started with 6 and then I move to 3.33. So, you see it is converging very fast 3.33. Then at 3.33 the value of f x 1 is 7.09 here if you compute this, the derivative will be twice of this that is 6.66, derivative is 2 x, 6.66. If I subtract this 6.66 divided by 7.09 subtracted from 3.33 is 2.27. So, my dx is 1.06. I go on like this and ultimately I come to a dx of 0.01. I assume. So, next time it is 2.27. I start with that again find the next value to be 2 point instead of 2.27 it will be 2.01. And with 2.01 I compute and gradually you see the dx is coming down. As the dx is coming down, that means I am approaching the actual route. So, this is Newton's Rapps and Newton Rapps and method and we can very easily code it. And so, here is an example. Now, you can see this, it starts with 6 goes to 3.33. Then from the 3 here is a little animation 3.33. I am coming to 2.2.7. Then from 2.2.7 I am coming here and gradually the error is that it is not increasing. So, it is very much converging. So, I get the solution with 2.01 and I get the solution. So, quickly the algorithm will look like this. Again I will read x the a maximum error allowed number of iterations and d. d is the checking for checking the slope. Here the comments, x is the initial gas, absolute error is e, n is the number of iterations. So, do in a loop i to n in steps of 2 f x, f is equal to f x and then f 1 is f prime x. Now, these are two functions which you have to write. And if the now, why is, why am I keeping this check? Why am I keeping this check? f 1 which is a slope? If f dash x is too small, that means what? That the slope is nearly horizontal. That means, I am not going to get any suppose something is something like this. If I come to this point and try to find a slope of this, the slope will be very horizontal. So, this is not a good choice. In that case, I have to reduce it and come to a point where I can find a slope. Now, in that way I go on and find the whether the it is coming to the close to the root and go on. Now, if it goes on, the iteration goes on, say for example, it is possible that I am missing the loop. Coming close to that, say a curve like this, the slope of the curve was such that I was trying to come here and somehow I missed the root. I go to another point. So, that is another special case. I need not bother you with that right now. So, let us have a quick look at the program. The program will be again. So, here we are trying to find out the root of a function x log x. The function is x log x to the base 10 minus 1.2. So, that is the function. That function is m body in another c function. Now, df is returning, df is nothing but f dash x. So, if this function is given, I also keep f dash x written. So, this is f x, this is f dash x. Now, I know I have already precoded them and that will return with the value for different values of x. So, now again I read as can F, I read the initial x 0, the allowed error, the maximum iteration. Now, then in this loop, what I do? I find f x by f dash x and that is h. How much I should reduce? How much I should change the initial value? The initial value was x 0 with which I started. I subtract that and come to the next point. And if the absolute error is less than absolute value at that point, if absolute at that point is less than error, then that is the solution. Otherwise, I will go up and repeat this. Now, if I go on and ultimately, if I overshoot the maximum iteration, then I can say that the required solution does not convert or the iterations are inefficient. So, Newton's reaction usually gives us a very fast way of finding the root, but sometimes it does not converge. And that is one problem of that. However, there are many other sophisticated ways of finding roots, just to summarize, I would like to say that what we have learnt in the past couple of lectures is that one of the major technological requirements, computational requirements are finding roots of polynomials for many solutions or many engineering solutions. I have to solve equations. For that, there are many methods we have just gone through to simpler methods. One is the bisection method and the other one that we saw just now is the Newton-Raphson method. Next, we will look at something else called interpolation and other things. Thank you.